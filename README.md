# RALO — Reinforcement Asynchronous Learning Orchestrator

RALO is an asynchronous Trainer–Sampler system for reinforcement learning built around a central Orchestrator. The Orchestrator owns the problem queue, sample queue, gradient aggregation, optimizer, and weight/version storage. Samplers fetch problems directly from the orchestrator, upload rollouts, and immediately pull consolidated weights; trainers only stream gradients. This centralized architecture ensures consistent model updates, efficient resource management, and scalable distributed training.

## Architecture
- **ProblemProvider**: preloads the orchestrator with `train_data × epochs`, serves `/problem/get`.
- **SampleQueueManager**: bounded queue + in-flight ledger between samplers and trainers.
- **GradientAggregator**: averages gradients, runs optimizer steps every `update_steps`, snapshots/prunes weights.
- **SamplerClient (vLLM workers)**: fetch problem → generate → `POST /upload` → pull `/weights/*`.
- **TrainerClient (DDP loop)**: `GET /get` batches → compute gradients → `POST /gradient/upload_chunk` + `POST /gradient/upload_finalize` → `/stats`/`/step/next`/`/lock/*`.
  - **DDP (Distributed Data Parallel)**: PyTorch's distributed training mode. Multiple GPUs/nodes run the same model in parallel, with each GPU processing different data batches. Launched via `torchrun --nproc_per_node=N` where N processes each handle one GPU.
- **Multi-threaded HTTP Server**: The orchestrator uses a multi-threaded Bottle server (WSGIRefServer with ThreadingMixIn) to handle concurrent requests from multiple Trainers and Samplers simultaneously, improving throughput and reducing timeout issues.

Flow:
```
Problem dataset ─▶ Orchestrator (ProblemProvider)
Samplers ──(GET /problem/get)───────────┐
Samplers ──(POST /upload)───────────────┤
                                        ├─ SampleQueueManager ──(GET /get)── Trainers
Trainers ──(POST /gradient/upload_chunk + /gradient/upload_finalize)───▶ GradientAggregator ── optimizer/weights
Samplers & Trainers ──(GET /weights/*) / (POST /weights/*) to stay in sync
```

### Why Use a Central Orchestrator?

RALO uses a centralized Orchestrator architecture for the following benefits:

1. **Centralized Gradient Aggregation**: The Orchestrator collects and averages gradients from multiple Trainers, then performs optimizer steps to update the global model. This ensures consistent model updates across all distributed workers (A3C-style centralized updates).

2. **Unified Sample Queue Management**: The Orchestrator manages a central queue of samples generated by Samplers and distributes them to Trainers. This enables efficient flow control between sample generation and consumption.

3. **Weight Version Management**: The Orchestrator maintains versioned model weights, ensuring all Trainers and Samplers use the same model version for consistency.

4. **Centralized Problem Data**: Problem datasets are managed by the Orchestrator, allowing Samplers to fetch problems via API without needing local dataset access.

5. **Scalability and Flexibility**: The Orchestrator enables independent scaling of Trainers and Samplers, with clear separation of concerns between components.

### Key Architectural Characteristics

RALO's Orchestrator architecture has the following key characteristics:

- **Centralized Optimizer Steps**: Only the Orchestrator performs optimizer steps. Trainers only compute and send gradients; they do not perform optimizer steps.

- **Gradient-based Updates**: Trainers collect gradients every `accum_steps` and send them to the Orchestrator. The Orchestrator averages gradients when `update_steps` gradients are collected, then performs an optimizer step to update the global model.

- **Clear Trainer Role Separation**: Trainer's role is Forward → Backward → **Send Gradients** → Download Weights. Optimizer steps are performed only by the Orchestrator.

- **Full Multi-Trainer Support**: A3C (Asynchronous Advantage Actor-Critic) style architecture where multiple asynchronous Trainer nodes independently collect experiences and the central Orchestrator updates the global network.

- **Global Model Consistency**: The Orchestrator maintains a single global model, ensuring all Trainers and Samplers use the same model version for consistency.

### Disk-Based Gradient Storage

RALO implements a disk-based gradient storage strategy to handle large-scale models (e.g., 7B+ parameters) without running out of RAM on the orchestrator.

**Key Benefits:**
1.  **Memory Efficiency**: Storing 100+ gradients for a 7B model in RAM would require hundreds of GBs. Disk-based storage keeps RAM usage constant regardless of the number of pending gradients.
2.  **Stability**: Prevents OOM (Out Of Memory) crashes on the orchestrator during long training runs.
3.  **Scalability**: Allows scaling to larger models and larger batch sizes (more `update_steps`) limited only by disk space.

**Mechanism:**

RALO uses a **disk-based gradient storage and processing pipeline** to handle large-scale models efficiently:

1. **Chunked Uploads**: 
   - Trainers split large gradients into chunks (default: 50MB per chunk) and upload them sequentially via `/gradient/upload_chunk` endpoint.
   - Each chunk is immediately saved to disk in `gradient_chunks_dir` to prevent RAM accumulation.
   - The orchestrator tracks chunk metadata (upload_id, chunk indices, timestamps) in memory, but the actual gradient data stays on disk.

2. **Reassembly**:
   - When all chunks for a gradient are received, the trainer calls `/gradient/upload_finalize` endpoint.
   - The orchestrator reads all chunk files from disk, concatenates them in order, and reassembles the complete gradient.
   - The reassembled gradient is saved as a single file in `gradient_storage_dir` with metadata (worker_id, step_id, batch_id, etc.).
   - **Chunk files are immediately deleted** after reassembly to free disk space.

3. **Incremental Accumulation**:
   - When `update_steps` gradients are collected, the optimizer step is triggered.
   - The orchestrator loads gradient files **one by one** from `gradient_storage_dir`.
   - For each file:
     - Load the gradient from disk
     - Accumulate it directly into model parameter gradients (on CPU)
     - **Immediately delete the file** after processing
   - This ensures that at most one gradient is in memory at any time, keeping RAM usage constant.

4. **Asynchronous Processing**:
   - Optimizer steps are performed in a background thread to avoid blocking HTTP requests.
   - The orchestrator continues accepting new gradient uploads while processing optimizer steps.

5. **Automatic Cleanup**:
   - Stale chunk files (older than `chunk_timeout`) are automatically cleaned up to prevent disk space leaks.
   - Reassembled gradient files are deleted immediately after being processed during optimizer steps.
   - Disk usage is capped at `max_gradient_disk_mb` (default: 1TB), with oldest files removed when exceeded.

**Configuration Parameters:**
- `gradient_chunks_dir` (default: auto-generated): Directory for storing gradient chunk files during upload. Defaults to `./orchestrator_gradient_chunks_{port}`.
- `gradient_storage_dir` (default: auto-generated): Directory for storing reassembled gradient files. Defaults to `./orchestrator_gradients_{port}`.
- `max_gradient_disk_mb` (default: 1024000.0MB = 1TB): Maximum disk usage for gradient files. Automatically removes oldest files when exceeded.
- `max_chunk_disk_mb` (default: 1024000.0MB = 1TB): Maximum disk usage for gradient chunk files. Automatically removes oldest chunks when exceeded.

**Why Disk-Based Only:**
- **RAM-based gradient storage has been removed** to simplify the architecture and ensure consistent behavior.
- All gradients are now processed through the disk-based path, which provides:
  - **Predictable memory usage**: RAM usage remains constant regardless of `update_steps` or model size.
  - **Scalability**: Can handle 100+ pending gradients without OOM errors.
  - **Reliability**: Prevents OOM crashes that could occur with RAM-based storage on large models.

**Performance Considerations:**
- **I/O Overhead**: Disk-based storage introduces I/O overhead (~5-10% slower) but prevents OOM crashes.
- **Memory Savings**: RAM usage remains constant regardless of `update_steps`, allowing scaling to very large batch sizes.
- **Disk Space**: Ensure sufficient disk space (recommended: `update_steps × average_gradient_size_mb × 2`).
- **Disk Speed**: Use fast SSD for `gradient_chunks_dir` and `gradient_storage_dir` for better performance.


## Quickstart
Prereqs: CUDA, PyTorch 2.0+, transformers, vLLM, bottle, datasets.

1. **Prepare a config**
   ```bash
   cp configs/example.yaml my_exp.yaml
   ```
   - `dataset` selects which HF split/problems the orchestrator preloads.
   - `sampler` / `trainer` `params` flow directly into their algorithms (generation length, accum_steps, device lists, etc.).
   - `wandb` toggles logging and sets project/run metadata.

## Running with Shell Scripts (Recommended for Background Processes)

Shell scripts provide a convenient way to run processes in the background with automatic logging to files. All logs are saved to `logs/{run_id}/` directory with unique run IDs.

### Start Orchestrator

```bash
# Using shell script (recommended)
./scripts/start_orch.sh \
  --config my_exp.yaml \
  --orch-port 59888 \
  --orch-host 0.0.0.0

# Or with environment variables
export ORCH_HOST=0.0.0.0
export ORCH_PORT=59888
./scripts/start_orch.sh --config my_exp.yaml
```

### Start Sampler

```bash
# Using shell script
./scripts/start_sampler.sh \
  --config my_exp.yaml \
  --orchestrator http://<server-ip>:59888 \
  --gen-devices 0,1,2,3 \
  --run-id <run_id>

# Or with environment variables
export ORCH_SERVER=http://<server-ip>:59888
export GEN_DEVICES=0,1,2,3
./scripts/start_sampler.sh --config my_exp.yaml --run-id <run_id>
```

### Start Trainer

```bash
# Using shell script (with torchrun for distributed training)
./scripts/start_trainer.sh \
  --config my_exp.yaml \
  --orchestrator http://<server-ip>:59888 \
  --cuda-visible-devices 0,1,2,3 \
  --nproc-per-node 4 \
  --master-port 29501 \
  --skip-trainer-registration 1 \
  --torch-nccl-async-error-handling 1 \
  --run-id <run_id>

# Or with environment variables
export ORCH_SERVER=http://<server-ip>:59888
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
export SKIP_TRAINER_REGISTRATION=1
export CUDA_VISIBLE_DEVICES=0,1,2,3
./scripts/start_trainer.sh \
  --config my_exp.yaml \
  --nproc-per-node 4 \
  --master-port 29501 \
  --run-id <run_id>
```

### Start All Processes

```bash
# Start orchestrator, trainer, and sampler together
./scripts/start_all.sh \
  --config my_exp.yaml \
  --orch-port 59888 \
  --orch-host 0.0.0.0 \
  --orchestrator http://<server-ip>:59888 \
  --gen-devices 0,1,2,3 \
  --cuda-visible-devices 0,1,2,3 \
  --nproc-per-node 4 \
  --master-port 29501 \
  --skip-trainer-registration 1 \
  --torch-nccl-async-error-handling 1
```

### Process Management

```bash
# Check status of all processes
./scripts/status.sh --log-dir logs/<run_id>

# Stop all processes in a specific run
./scripts/stop_all.sh --log-dir logs/<run_id>

# Stop all processes in all log directories (wildcard expansion)
./scripts/stop_all.sh --log-dir logs/*

# Stop all processes matching a pattern (quoted wildcard)
./scripts/stop_all.sh --log-dir 'logs/20251120_*'
```

**Log Files:**
- Logs are saved to `logs/{run_id}/` with separate files for each process:
  - `orchestrator.log` - Orchestrator stdout/stderr
  - `trainer.log` - Trainer stdout/stderr  
  - `sampler.log` - Sampler stdout/stderr
- PID files: `orchestrator.pid`, `trainer.pid`, `sampler.pid`
- Run ID is saved to `run_id.txt`

## Running with SLURM (Cluster Environment)

RALO provides SLURM scripts for running on HPC clusters. These scripts handle job submission, resource allocation, and automatic logging.

### Prerequisites

- SLURM workload manager
- CUDA modules loaded
- Python virtual environment activated
- All scripts are in `scripts/` directory

### Submit All Jobs (Recommended)

The easiest way to run all components is using the `slurm_submit_all.sh` script:

```bash
# Set environment variables
export CONFIG_FILE=my_exp.yaml
export ORCHESTRATOR_URL=http://<orchestrator-host>:59888

# Submit all jobs (orchestrator, trainer, sampler)
bash scripts/slurm_submit_all.sh
```

**Customizing Partition and GPU Settings:**

You can override partition and GPU settings via environment variables:

```bash
# Override partition and GPU type
export SLURM_PARTITION=gpu3
export SLURM_GPU_TYPE=a6000ada
export SLURM_GPU_COUNT_TRAINER=4
export SLURM_GPU_COUNT_SAMPLER=4
# Note: Orchestrator runs on CPU only, no GPU required

# Submit with custom settings
export CONFIG_FILE=my_exp.yaml
export ORCHESTRATOR_URL=http://<orchestrator-host>:59888
bash scripts/slurm_submit_all.sh
```

This script will:
1. Generate a shared `run_id` for all components
2. Submit orchestrator job first
3. Wait 10 seconds for orchestrator to start
4. Submit trainer and sampler jobs
5. Save all job IDs to log directory

### Submit Individual Jobs

You can also submit jobs individually:

#### Orchestrator

```bash
export CONFIG_FILE=my_exp.yaml
export RUN_ID=my_run_123
export ORCH_PORT=59888
export ORCH_HOST=0.0.0.0

# Default: partition=gpu4, CPU only (no GPU required)
sbatch scripts/slurm_orch.sh

# Or override partition (orchestrator runs on CPU only):
SLURM_PARTITION=cpu \
sbatch --partition=$SLURM_PARTITION scripts/slurm_orch.sh
```

#### Trainer

```bash
export CONFIG_FILE=my_exp.yaml
export RUN_ID=my_run_123
export ORCHESTRATOR_URL=http://<orchestrator-host>:59888
export NPROC_PER_NODE=4
export MASTER_PORT=29501

# Default: partition=gpu4, GPU=a6000:4
sbatch scripts/slurm_trainer.sh

# Or override partition and GPU via sbatch options:
SLURM_PARTITION=gpu3 SLURM_GPU_TYPE=a6000ada SLURM_GPU_COUNT_TRAINER=4 \
sbatch --partition=$SLURM_PARTITION --gres=gpu:$SLURM_GPU_TYPE:$SLURM_GPU_COUNT_TRAINER scripts/slurm_trainer.sh
```

#### Sampler

```bash
export CONFIG_FILE=my_exp.yaml
export RUN_ID=my_run_123
export ORCHESTRATOR_URL=http://<orchestrator-host>:59888
export GEN_DEVICES=0,1,2,3

# Default: partition=gpu4, GPU=a6000:4
sbatch scripts/slurm_sampler.sh

# Or override partition and GPU via sbatch options:
SLURM_PARTITION=gpu3 SLURM_GPU_TYPE=a6000ada SLURM_GPU_COUNT_SAMPLER=4 \
sbatch --partition=$SLURM_PARTITION --gres=gpu:$SLURM_GPU_TYPE:$SLURM_GPU_COUNT_SAMPLER scripts/slurm_sampler.sh
```

### Stop SLURM Jobs

```bash
# Stop all jobs for a specific run
bash scripts/slurm_stop.sh --log-dir logs/<run_id>

# Stop all jobs in all log directories (wildcard expansion)
bash scripts/slurm_stop.sh --log-dir logs/*

# Stop all jobs matching a pattern (quoted wildcard)
bash scripts/slurm_stop.sh --log-dir 'logs/20251120_*'
```

**Note:** The scripts support both wildcard expansion (when shell expands `logs/*`) and pattern matching (when using quotes `'logs/*'`). Both methods work correctly.

### SLURM Script Configuration

All SLURM scripts use the following default settings:

- **Partition**: `gpu4` (can be overridden via `SLURM_PARTITION` environment variable)
- **Nodes**: 1
- **CPUs per task**: 4
- **GPUs**: 
  - Orchestrator: **CPU only** (no GPU required - runs optimizer steps on CPU)
  - Trainer: 4 GPUs of type `a6000` with 4 processes per node (can be overridden via `SLURM_GPU_TYPE` and `SLURM_GPU_COUNT_TRAINER`)
  - Sampler: 4 GPUs of type `a6000` (can be overridden via `SLURM_GPU_TYPE` and `SLURM_GPU_COUNT_SAMPLER`)

**Customization Options:**

1. **Via Environment Variables with `slurm_submit_all.sh` (Recommended)**: Set environment variables before running `slurm_submit_all.sh`:
   ```bash
   export SLURM_PARTITION=gpu3
   export SLURM_GPU_TYPE=a6000ada
   export SLURM_GPU_COUNT_TRAINER=4
   export SLURM_GPU_COUNT_SAMPLER=4
   # Note: Orchestrator runs on CPU only, no GPU required
   bash scripts/slurm_submit_all.sh
   ```

2. **Via Environment Variables with Individual Scripts**: Use environment variables with `sbatch` options when submitting individual scripts:
   ```bash
   # For orchestrator (CPU only)
   SLURM_PARTITION=cpu \
   sbatch --partition=$SLURM_PARTITION scripts/slurm_orch.sh
   
   # For trainer
   SLURM_PARTITION=gpu3 SLURM_GPU_TYPE=a6000ada SLURM_GPU_COUNT_TRAINER=4 \
   sbatch --partition=$SLURM_PARTITION --gres=gpu:$SLURM_GPU_TYPE:$SLURM_GPU_COUNT_TRAINER scripts/slurm_trainer.sh
   
   # For sampler
   SLURM_PARTITION=gpu3 SLURM_GPU_TYPE=a6000ada SLURM_GPU_COUNT_SAMPLER=4 \
   sbatch --partition=$SLURM_PARTITION --gres=gpu:$SLURM_GPU_TYPE:$SLURM_GPU_COUNT_SAMPLER scripts/slurm_sampler.sh
   ```

3. **Edit Script Files**: Modify the `#SBATCH` directives in each script file (`slurm_orch.sh`, `slurm_trainer.sh`, `slurm_sampler.sh`).

### SLURM Log Files

- SLURM output: `slurm_logs/{process}.{node}.{jobid}.out`
- SLURM errors: `slurm_logs/{process}.{node}.{jobid}.err`
- Application logs: `logs/{run_id}/{process}_node{nodeid}.log`
- Job IDs: `logs/{run_id}/{process}_slurm_jobid.txt`
- SLURM info: `logs/{run_id}/{process}_slurm_info.txt`

### Environment Variables

SLURM scripts support the following environment variables:

**Application Configuration:**
- `CONFIG_FILE`: Path to YAML config file (default: `my_exp.yaml`)
- `LOG_DIR`: Log directory (default: `logs/{run_id}`)
- `RUN_ID`: Run ID (auto-generated if not provided)
- `ORCHESTRATOR_URL`: Orchestrator URL (required for trainer/sampler)
- `ORCH_PORT`: Orchestrator port (default: 59888)
- `ORCH_HOST`: Orchestrator host (default: 0.0.0.0)
- `NPROC_PER_NODE`: Number of processes per node for trainer (default: 4)
- `MASTER_PORT`: Master port for distributed training (default: 29501)
- `GEN_DEVICES`: GPU devices for sampler (auto-detected from SLURM if not set)

**SLURM Resource Configuration (for `slurm_submit_all.sh`):**
- `SLURM_PARTITION`: SLURM partition name (default: `gpu4`)
- `SLURM_GPU_TYPE`: GPU type (default: `a6000`)
- `SLURM_GPU_COUNT_TRAINER`: Number of GPUs for trainer (default: `4`)
- `SLURM_GPU_COUNT_SAMPLER`: Number of GPUs for sampler (default: `4`)
- **Note**: Orchestrator runs on CPU only, no GPU required

**Example:**
```bash
# Customize SLURM resources
export SLURM_PARTITION=gpu3
export SLURM_GPU_TYPE=a6000ada
export SLURM_GPU_COUNT_TRAINER=8
# Orchestrator runs on CPU only, no GPU required

# Application settings
export CONFIG_FILE=my_exp.yaml
export ORCHESTRATOR_URL=http://<orchestrator-host>:59888

# Submit jobs
bash scripts/slurm_submit_all.sh
```

### Monitoring SLURM Jobs

```bash
# Check job status
squeue -u $USER

# Check specific job IDs
squeue -j <job_id1>,<job_id2>,<job_id3>

# View job details
scontrol show job <job_id>

# View job output
tail -f slurm_logs/orch.*.out
tail -f slurm_logs/trainer.*.out
tail -f slurm_logs/sampler.*.out
```

### Notes

- **Orchestrator must start first**: The orchestrator job should be submitted and running before trainer/sampler jobs
- **Network configuration**: For multi-node setups, ensure `ORCHESTRATOR_URL` uses the actual node hostname/IP
- **GPU allocation**: Scripts automatically detect GPUs allocated by SLURM via `SLURM_STEP_GPUS`
- **Memory optimization**: Trainer script includes `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` to reduce memory fragmentation
- **Node identification**: Log files include node information (e.g., `trainer_node0.log`) for easy identification

## Running with Python Directly

You can also run processes directly with Python for foreground execution:

### Start Orchestrator

```bash
export ORCH_HOST=0.0.0.0
export ORCH_PORT=59888
python ralo_cli.py orch --config my_exp.yaml
```

### Start Trainer

```bash
export ORCH_SERVER=http://<server-ip>:59888
TORCH_NCCL_ASYNC_ERROR_HANDLING=1 \
SKIP_TRAINER_REGISTRATION=1 \
CUDA_VISIBLE_DEVICES=0,1,2,3 \
torchrun --standalone --nproc_per_node=4 --master_port=29501 \
  ralo_cli.py train --config my_exp.yaml
```

### Start Sampler

```bash
export ORCH_SERVER=http://<server-ip>:59888
export GEN_DEVICES=0,1,2,3
python ralo_cli.py gen --config my_exp.yaml
```

**Tips:**
- `ralo_cli.py` defaults to the `train` command, so `torchrun ... ralo_cli.py --config my_exp.yaml` works even without explicitly passing `train`.
- `--orchestrator=http://host:port` overrides both YAML and the `ORCH_SERVER` env var.
- CLI flags > env vars > YAML > built-in defaults.
- Use `--log-dir` and `--run-id` to organize logs when running directly with Python.

## Hyperparameter Configuration & Relationships

All hyperparameters can be controlled via YAML configuration files. This section explains the key parameters and their interrelationships.

### Complete YAML Configuration Structure

```yaml
model_path: Qwen/Qwen2.5-7B
orchestrator_port: 59888
update_steps: 128  # Orchestrator: gradient uploads before optimizer step
lr: 1.0e-6  # Learning rate (used by orchestrator optimizer)
epochs: 10

sampler:
  algorithm: treepo
  params:
    rollout_num: 16  # Number of rollouts per problem
    train_batch_size: 1  # Batch size for training (must divide rollout_num)
    gen_max_tokens: 1024  # Maximum tokens to generate per sample
    gen_temperature: 0.8  # Sampling temperature (0.0-2.0, higher = more random)
    max_pending_samples: 12800  # Pause generation when queue has this many samples
    gen_pending_time: 10.0  # Wait time (seconds) when queue is full
    version_poll_interval: 5.0  # Interval in seconds to check for new weight versions (default: 5 seconds)
    max_batch_retry: 3  # Maximum retry count for batch processing failures (default: 3)
    treepo_kwargs:
      generation_length: 1024
      depth: 7
      budget_coefficient: 2
      sampling_batch_size: 16

trainer:
  algorithm: treepo
  params:
    rollout_num: 16
    train_batch_size: 1
    accum_steps: 64  # Local gradient accumulation before sending to orchestrator
    lr: 1.0e-6  # Learning rate (should match orchestrator lr)
    grad_offload: true  # Enable gradient offloading to CPU to prevent OOM during backward pass (recommended for long sequences 14K+)
    gradient_checkpointing_ratio: 1.0  # Ratio of layers to enable gradient checkpointing (1.0 = all layers, 0.5 = last 50% layers)
    max_batch_retry: 3  # Maximum retry count for batch processing failures (default: 3)
    clip_param: 0.2  # PPO clipping parameter (default: 0.2)
    pending_retry_timeout: 360.0  # Timeout in seconds for pending batch retry (default: 360 seconds = 6 minutes)

orchestrator:
  batch_timeout: 3600.0  # Timeout for batch processing (seconds)
  problem_timeout: 600.0  # Timeout for problem processing (seconds, default: 10 minutes)
  queue_size: 1600  # Maximum training queue size
  status_report_interval: 30.0  # Interval in seconds for status report output (default: 30 seconds)
  lock_ttl: 30.0  # Lock Time-To-Live in seconds (default: 30 seconds)
  server_threads: 10  # Number of threads for handling concurrent requests (default: 10)
  timeout_check_interval: 60.0  # How often to check for timeouts (seconds)
  keep_last_versions: 2  # Number of weight versions to keep on disk
  chunk_size_mb: 50  # Chunk size in MB for gradient uploads (default: 50MB)
  download_chunk_size_mb: 32  # Chunk size in MB for weight downloads (default: 32MB)
  # Gradient chunk management (memory leak prevention)
  chunk_timeout: 600.0  # Timeout in seconds for stale gradient chunks (default: 10 minutes)
  max_concurrent_uploads: 50  # Maximum concurrent gradient chunk uploads (default: 50)
  chunk_cleanup_interval: 60.0  # Interval in seconds for chunk cleanup (default: 60 seconds)
  # Disk-based gradient storage (for large models)
  gradient_chunks_dir: null  # Directory for gradient chunk files (default: auto-generated as ./orchestrator_gradient_chunks_{port})
  gradient_storage_dir: null  # Directory for restored gradient files (default: auto-generated as ./orchestrator_gradients_{port})
  max_gradient_disk_mb: 1024000.0  # Maximum disk usage in MB for gradient files (default: 1TB)
  max_chunk_disk_mb: 1024000.0  # Maximum disk usage in MB for gradient chunk files (default: 1TB)
  # HTTP request timeouts (in seconds)
  get_batch_timeout: 60.0  # Timeout for trainer get_batch requests (default: 60 seconds)
  send_gradients_timeout: 300.0  # Timeout for gradient upload requests (default: 5 minutes)
  download_weights_timeout: 600.0  # Timeout for weight download requests (default: 10 minutes)
  upload_samples_timeout: 300.0  # Timeout for sample upload requests (default: 5 minutes)
  fetch_problem_timeout: 10.0  # Timeout for problem fetch requests (default: 10 seconds)
  register_timeout: 10.0  # Timeout for trainer registration (default: 10 seconds)
  stats_timeout: 5.0  # Timeout for stats requests (default: 5 seconds)
  heartbeat_timeout: 2.0  # Timeout for heartbeat requests (default: 2 seconds)
  version_check_timeout: 5.0  # Timeout for version check requests (default: 5 seconds)
  next_step_timeout: 5.0  # Timeout for next_step requests (default: 5 seconds)
  lock_timeout: 5.0  # Timeout for lock acquire/release requests (default: 5 seconds)
  # Log control configuration (disable specific log categories to reduce output)
  log_control:
    log_sample_upload: true  # Enable sample upload logs (default: true)
    log_batch_dispatch: true  # Enable batch dispatch logs (default: true)
    log_gradient_received: true  # Enable gradient received logs (default: true)
    log_gradient_reassembled: true  # Enable reassembled gradient logs (default: true)
    log_gradient_chunks: true  # Enable gradient chunks progress logs (default: true)
    log_optimizer_step: true  # Enable optimizer step completion logs (default: true)
    log_processing_gradient: true  # Enable processing reassembled gradient logs (default: true)
    log_status_report: true  # Enable periodic status report logs (default: true)
    log_http_access: true  # Enable HTTP access logs (WSGI server logs) (default: true)

wandb:
  enabled: true  # Enable/disable wandb logging
  project: entropy seesaw  # Wandb project name
  run_name: TreePO_experiment  # Wandb run name
  tags: ["demo"]  # Optional tags for wandb run
  # entity: your-entity  # Optional: wandb entity/team name

dataset:
  name: qwedsacf/competition_math  # HuggingFace dataset name
  split: train  # Dataset split to use
  filter_levels: ["Level 3", "Level 4", "Level 5"]  # Filter problems by difficulty level
  shuffle_seed: 42  # Random seed for dataset shuffling
```

### Parameter Relationships & Flow Control

#### 1. **Training Flow: Sampler → Orchestrator → Trainer**

**Sampler Parameters:**
- `max_pending_samples` (default: 12800): Flow control threshold. When orchestrator's queue reaches this size, sampler pauses generation for `gen_pending_time` seconds.
- `gen_pending_time` (default: 10.0): Wait time when queue is full.
- **Relationship**: `max_pending_samples` should be ≥ `orchestrator.queue_size` to prevent premature pausing. If `max_pending_samples < queue_size`, sampler may pause even when queue has space.
- `compute_report_interval` (default: 60s) & `compute_report_token_threshold` (default: 32k tokens) control how frequently each sampler worker flushes GPU-seconds/tokens telemetry to the orchestrator.

**Orchestrator Parameters:**
- `queue_size` (default: 1600): Hard limit on training queue. When full, new samples are rejected.
- **Relationship**: `queue_size` acts as the bottleneck. If sampler generates faster than trainers consume, queue fills up and sampler pauses.

#### 2. **Gradient Accumulation & Update Flow**

**Trainer Parameters:**
- `accum_steps` (default: varies): Number of micro-batches to accumulate locally before sending gradient to orchestrator.
- **Effect**: Each trainer accumulates `accum_steps` micro-batches, then sends **1 gradient upload** to orchestrator.
- `compute_report_interval` / `compute_report_token_threshold`: govern how often each trainer rank reports learner GPU-seconds/tokens (defaults match the sampler settings).

**Orchestrator Parameters:**
- `update_steps` (default: 128): Number of **gradient uploads** (not micro-batches) needed before optimizer step.
- **Relationship**: 
  - If you have 4 trainers, each with `accum_steps=64`:
    - Each trainer sends 1 gradient upload per 64 micro-batches
    - Orchestrator needs `update_steps=128` gradient uploads total
    - This means 128 trainer cycles = 128 × 64 = 8192 micro-batches globally
  - **Global effective batch size**: `update_steps × accum_steps × num_trainers` (if all trainers have same `accum_steps`)

**Learning Rate:**
- `lr` in both trainer and orchestrator configs should match (orchestrator's optimizer uses this value).

#### 3. **Batch Size Relationships**

**Global Batch Size Formula:**
```
global_batch = train_batch_size × world_size × accum_steps × (update_steps / num_trainers)
```

**Scaling Rules:**
- **Keep global batch constant when scaling GPUs**: 
  - 1 GPU → 2 GPUs: halve `accum_steps` OR halve `update_steps`
- **Increase global batch with more GPUs**: 
  - Keep `accum_steps` and `update_steps` constant, or increase them proportionally
- **Memory constraints**: 
  - Increasing `train_batch_size` can cause OOM (per-GPU memory increases)
  - Increasing `accum_steps` does NOT increase per-GPU memory (gradients accumulate in-place)
  - Distributed training does NOT increase per-GPU memory

#### 4. **Memory Optimization Parameters**

RALO includes advanced memory optimization techniques inspired by [LSRL](https://github.com/lsdefine/lsrl) to enable training long sequences (14K+) on limited GPU memory.

**Gradient Offloading:**
- `grad_offload` (default: false): Enable immediate gradient offloading to CPU after backward pass.
- **Effect**: 
  - Gradients are immediately moved to CPU after `backward()`, freeing GPU memory during gradient accumulation.
  - Prevents OOM errors when training long sequences or using large `accum_steps`.
  - Optimizer states are stored on CPU, reducing GPU memory usage.
- **When to use**: 
  - **Recommended for sequences ≥ 14K tokens**
  - **Recommended when `accum_steps` ≥ 64**
  - **Required when experiencing OOM during backward pass**
- **Performance**: Minimal overhead (~5-10%) due to asynchronous CPU transfers.
- **Technical details**: 
  - Uses CPUAdamW optimizer with CPU-based optimizer states
  - Gradients are offloaded immediately after backward, before accumulation completes
  - Periodic activation cleanup during accumulation prevents memory leaks

**Gradient Checkpointing:**
- `gradient_checkpointing_ratio` (default: 1.0): Ratio of transformer layers to enable gradient checkpointing.
- **Effect**: 
  - Trades computation for memory by recomputing activations during backward pass.
  - `1.0` = all layers use checkpointing (maximum memory savings, ~30% slower)
  - `0.5` = last 50% of layers use checkpointing (balanced)
  - `0.0` = no checkpointing (fastest, highest memory usage)
- **When to use**: 
  - Enable when GPU memory is limited
  - Use `1.0` for very long sequences or large models
  - Use `0.5` for balanced memory/computation trade-off
- **Memory savings**: Can reduce activation memory by 50-70% depending on sequence length.

**Memory Optimization Flow:**
1. **Forward pass**: Model computes activations (with checkpointing if enabled)
2. **Backward pass**: Gradients computed, immediately offloaded to CPU if `grad_offload=true`
3. **Gradient accumulation**: Gradients accumulated on CPU, GPU memory freed
4. **Periodic cleanup**: Activations cleared every 25% of accumulation steps
5. **Optimizer step**: Performed on CPU (CPUAdamW), weights synced back to GPU

**Recommended Settings by Sequence Length:**
- **< 4K tokens**: `grad_offload: false`, `gradient_checkpointing_ratio: 0.0` (fastest)
- **4K-8K tokens**: `grad_offload: false`, `gradient_checkpointing_ratio: 0.5` (balanced)
- **8K-14K tokens**: `grad_offload: true`, `gradient_checkpointing_ratio: 0.5` (memory-efficient)
- **≥ 14K tokens**: `grad_offload: true`, `gradient_checkpointing_ratio: 1.0` (maximum memory savings)

#### 5. **Timeout & Reliability Parameters**

**Orchestrator Timeouts:**
- `batch_timeout` (default: 3600s = 1 hour): Maximum time a batch can be processed before being requeued.
- `problem_timeout` (default: 600s = 10 minutes): Maximum time a sampler can process a problem before it's requeued.
- `timeout_check_interval` (default: 60s): How often orchestrator checks for timed-out batches/problems.
- **Relationship**: `timeout_check_interval` should be < `batch_timeout` and < `problem_timeout` for timely failure detection.
- **Use case**: 
  - Increase `batch_timeout` if your batches take longer to process (e.g., very large models or slow GPUs).
  - Increase `problem_timeout` if problems are complex and require longer generation time.

**Weight Versioning:**
- `keep_last_versions` (default: 2): Number of weight versions to keep on disk.
- **Effect**: Older versions are automatically pruned to save disk space.

**Gradient Upload Chunking:**
- `chunk_size_mb` (default: 50MB): Chunk size for gradient uploads when gradients exceed this size.
- **Effect**: Large gradients are automatically split into chunks to avoid HTTP timeouts.
- **Use case**: Increase for faster networks, decrease for slower networks or to reduce memory usage during upload.

**Gradient Chunk Memory Management:**
- `chunk_timeout` (default: 1200.0s = 20 minutes): Timeout for stale gradient chunks that haven't been finalized.
- **Effect**: Prevents memory leaks by automatically cleaning up incomplete chunk uploads that exceed this timeout. Chunks older than this duration are automatically removed.
- **Use case**: 
  - Increase if gradient uploads take longer due to slow networks or large models.
  - Decrease to free memory faster if experiencing memory pressure.
  - Should be longer than expected gradient upload duration (typically 5-10 minutes for large models).
- `max_concurrent_uploads` (default: 200): Maximum number of concurrent gradient chunk uploads allowed.
- **Effect**: Limits the number of simultaneous chunk uploads to prevent memory exhaustion. When this limit is reached, new uploads are rejected with HTTP 503 status.
- **Use case**: 
  - Increase for large-scale deployments with many trainers (e.g., 16+ GPUs).
  - Decrease if experiencing memory pressure or OOM errors.
  - Recommended: Number of Trainers × 2-4 (to account for retries and concurrent batches).
- `max_chunk_disk_mb` (default: 1024000.0MB = 1TB): Maximum disk usage in MB for gradient chunk files.
- **Effect**: Automatically removes oldest chunk files when disk usage exceeds this limit. This prevents disk space exhaustion from incomplete uploads.
- **Use case**: 
  - Increase for systems with large disk space (e.g., 2TB+).
  - Decrease if disk space is limited.
  - Should be set based on: `(max_concurrent_uploads × chunk_size_mb × 2)` to allow buffer.
- `chunk_cleanup_interval` (default: 60.0s = 1 minute): Interval in seconds for periodic cleanup of stale gradient chunks.
- **Effect**: Controls how frequently the orchestrator checks for and removes stale chunks. More frequent cleanup reduces memory usage but increases CPU overhead.
- **Use case**: 
  - Decrease (e.g., 30s) for aggressive memory management or when experiencing memory pressure.
  - Increase (e.g., 120s) to reduce CPU overhead in stable environments.
- **Memory Leak Prevention**: These parameters work together to prevent disk space leaks from incomplete gradient uploads:
  - Chunks are automatically cleaned up after `chunk_timeout` seconds.
  - Disk usage for chunks is capped at `max_chunk_disk_mb` (default: 1TB).
  - Disk usage for reassembled gradients is capped at `max_gradient_disk_mb` (default: 1TB).
  - Cleanup runs every `chunk_cleanup_interval` seconds.
  - Status reports include chunk/disk usage for monitoring.

**Weight Download Chunking:**
- `download_chunk_size_mb` (default: 32MB): Chunk size for weight downloads.
- **Effect**: Weights are streamed in chunks to manage memory usage.
- **Use case**: Adjust based on network bandwidth and available memory.

**Disk-Based Gradient Storage:**
These parameters control the disk-based gradient storage system (recommended for large models):
- `gradient_chunks_dir` (default: auto-generated): Directory for storing gradient chunk files during upload.
  - Default: `./orchestrator_gradient_chunks_{port}`
  - **Effect**: Chunks are saved to disk instead of RAM, preventing memory exhaustion.
  - **Use case**: Specify a custom directory if you want to control disk location (e.g., fast SSD).
- `gradient_storage_dir` (default: auto-generated): Directory for storing reassembled gradient files.
  - Default: `./orchestrator_gradients_{port}`
  - **Effect**: Reassembled gradients are saved to disk before processing. Files are loaded one by one during optimizer step.
  - **Use case**: Specify a custom directory for better I/O performance (e.g., fast SSD).
- `max_gradient_disk_mb` (default: 1024000.0MB = 1TB): Maximum disk usage for reassembled gradient files.
  - **Effect**: Automatically removes oldest gradient files when disk usage exceeds this limit. This prevents disk space exhaustion.
  - **Use case**: 
    - Increase for systems with large disk space (e.g., 2TB+).
    - Decrease if disk space is limited.
    - Should be set based on: `(update_steps × average_gradient_size_mb × 2)` to allow buffer.
    - For 7B models with `update_steps=128`, typical gradient size is ~30GB, so set to at least `128 × 30 × 2 = 7680MB` (7.5GB) minimum, but 1TB recommended for safety.
  - **Note**: Gradient files are automatically deleted immediately after being processed during optimizer steps, so this limit primarily protects against edge cases.
- **Benefits**: 
  - Prevents OOM errors on orchestrator for large models (7B+ parameters).
  - Allows scaling to larger batch sizes (`update_steps`) limited only by disk space.
  - Keeps RAM usage constant regardless of number of pending gradients.
  - Enables training with 100+ pending gradients without memory issues.
  - **Automatic file cleanup**: Gradient files are deleted immediately after processing, keeping disk usage minimal.
- **Performance**: 
  - I/O overhead: ~5-10% slower than RAM-based storage, but prevents OOM crashes.
  - Disk space requirement: Ensure sufficient space based on `update_steps` and model size.
  - Recommended: Use fast SSD for `gradient_chunks_dir` and `gradient_storage_dir` for better performance.
  - **Processing flow**: Gradients are loaded one-by-one from disk, accumulated directly into model parameters, and files are deleted immediately, ensuring minimal disk usage.

**Retry & Reliability:**
- `max_batch_retry` (default: 3): Maximum number of retries for batch processing failures.
- **Effect**: Failed batches are retried up to this many times before being dropped.
- **Use case**: Increase in unstable network environments, decrease to fail faster in stable environments.
- `pending_retry_timeout` (default: 360.0s): Timeout in seconds for pending batch retry.
- **Effect**: Controls how long to wait before retrying a pending batch.
- **Use case**: Adjust based on expected batch processing time and network latency.

**Log Control:**
- `orchestrator.log_control`: Configuration section to control orchestrator log output verbosity.
- **Available flags** (all default to `true` for backward compatibility):
  - `log_sample_upload`: Enable sample upload logs (e.g., `[ORCH] Sample uploaded`)
  - `log_batch_dispatch`: Enable batch dispatch logs (e.g., `[ORCH] Batch dispatched to trainer`)
  - `log_gradient_received`: Enable gradient received logs (e.g., `[ORCH] Gradient received from ...`)
  - `log_gradient_reassembled`: Enable reassembled gradient logs (e.g., `[ORCH] Reassembled gradient`)
  - `log_gradient_chunks`: Enable gradient chunks progress logs (e.g., `[ORCH] Gradient chunks: X/Y`)
  - `log_optimizer_step`: Enable optimizer step completion logs (e.g., `[ORCH] ✓ Optimizer step completed`)
  - `log_processing_gradient`: Enable processing reassembled gradient logs (e.g., `[ORCH] Processing reassembled gradient`)
  - `log_status_report`: Enable periodic status report logs (e.g., `[ORCH STATUS] Step: ...`)
  - `log_http_access`: Enable HTTP access logs (WSGI server logs) (default: true)
- **Effect**: Disabling specific log categories reduces console output for easier monitoring.
- **Use case**: 
  - Set specific flags to `false` to reduce log noise during training
  - Keep `log_status_report: true` for periodic monitoring
  - Keep `log_optimizer_step: true` to track model updates
  - Disable verbose logs like `log_sample_upload` or `log_gradient_chunks` for cleaner output
- **Example configuration**:
  ```yaml
  orchestrator:
    log_control:
      log_sample_upload: false  # Disable frequent sample upload logs
      log_batch_dispatch: false  # Disable batch dispatch logs
      log_gradient_received: false  # Disable gradient received logs
      log_gradient_chunks: false  # Disable gradient chunks progress logs
      log_status_report: true  # Keep status reports enabled
      log_optimizer_step: true  # Keep optimizer step logs enabled
  ```
- **Note**: Error logs and critical system messages are always enabled regardless of these settings.

**Algorithm Parameters:**
- `clip_param` (default: 0.2): PPO clipping parameter for policy updates.
- **Effect**: Limits the magnitude of policy updates to prevent large policy changes.
- **Use case**: Standard PPO hyperparameter; typically 0.1-0.3 range.

**Orchestrator Logging & Locking:**
- `status_report_interval` (default: 30.0s): Interval in seconds for orchestrator status report output.
- **Effect**: Controls how frequently orchestrator prints status updates.
- **Use case**: Decrease for more frequent updates (more log output), increase to reduce log verbosity.
- `lock_ttl` (default: 30.0s): Lock Time-To-Live in seconds.
- **Effect**: Locks automatically expire after this duration if not renewed.
- **Use case**: Adjust based on expected operation duration; longer operations need longer TTL.
- `server_threads` (default: 10): Maximum number of concurrent threads for handling HTTP requests.
- **Effect**: Limits the number of simultaneous request handlers using a thread pool. Requests beyond this limit will queue until a thread becomes available.
- **Use case**: 
  - Increase for more concurrent clients (e.g., 20-50 for large-scale deployments).
  - Decrease if experiencing resource constraints (CPU/memory).
  - Recommended: Number of Trainers + Number of Samplers + 2-5 buffer.
- **Important**: This parameter actually limits the thread pool size, preventing resource exhaustion from too many concurrent requests.

**Weight Version Polling:**
- `version_poll_interval` (default: 5.0s): Interval in seconds to check for new weight versions from orchestrator.
- **Effect**: Samplers/trainers poll orchestrator for new weights at this interval.
- **Use case**: 
  - Decrease for faster weight synchronization (more network traffic).
  - Increase to reduce network load (slower synchronization).

#### 6. **Generation Parameters**

**Sampler Generation:**
- `gen_max_tokens` (default: 1024): Maximum tokens per generated sample.
- `gen_temperature` (default: 0.8): Sampling temperature (0.0 = deterministic, 2.0 = very random).
- `rollout_num` (default: 16): Number of rollouts per problem.
- **Relationship**: `train_batch_size` must divide `rollout_num` evenly.

#### 7. **Wandb Logging Configuration**

**Wandb Parameters:**
- `enabled` (default: true): Enable or disable wandb logging.
- `project` (default: "entropy seesaw"): Wandb project name where runs will be logged.
- `run_name` (default: "TreePO_experiment"): Name for this wandb run.
- `tags` (default: []): Optional list of tags for organizing runs in wandb.
- `entity` (optional): Wandb entity/team name. If not specified, uses your default wandb account.
- **Effect**: When enabled, training metrics, losses, and system stats are logged to wandb for experiment tracking.
- **Use case**: Set `enabled: false` to disable wandb logging, or use `WANDB_DISABLED=true` environment variable.

#### 8. **Dataset Configuration**

**Dataset Parameters:**
- `name` (default: "qwedsacf/competition_math"): HuggingFace dataset identifier to load.
- `split` (default: "train"): Dataset split to use (e.g., "train", "test", "validation").
- `filter_levels` (default: ["Level 3", "Level 4", "Level 5"]): List of difficulty levels to filter problems by.
  - Only problems matching these levels will be loaded into the orchestrator's problem queue.
- `shuffle_seed` (default: 42): Random seed for shuffling the dataset before training.
  - Use the same seed for reproducible experiments.
- **Effect**: The orchestrator preloads `dataset × epochs` problems into its queue based on these settings.
- **Use case**: 
  - Change `name` to use a different dataset.
  - Adjust `filter_levels` to focus on specific difficulty ranges.
  - Change `shuffle_seed` for different problem orderings.

#### 8. **HTTP Request Timeout Configuration**

**Orchestrator HTTP Timeouts:**
All HTTP request timeouts are configurable via `orchestrator` section. These control how long the system waits for various operations:

- `get_batch_timeout` (default: 60.0s): Timeout for trainer `GET /get` requests to fetch batches.
- `send_gradients_timeout` (default: 300.0s = 5min): Timeout for trainer `POST /gradient/upload` requests.
- `download_weights_timeout` (default: 600.0s = 10min): Timeout for weight download requests (large files).
- `upload_samples_timeout` (default: 300.0s = 5min): Timeout for sampler `POST /upload` requests.
- `fetch_problem_timeout` (default: 10.0s): Timeout for sampler `GET /problem/get` requests.
- `register_timeout` (default: 10.0s): Timeout for trainer registration requests.
- `stats_timeout` (default: 5.0s): Timeout for `GET /stats` requests.
- `heartbeat_timeout` (default: 2.0s): Timeout for trainer heartbeat requests.
- `version_check_timeout` (default: 5.0s): Timeout for weight version check requests.
- `next_step_timeout` (default: 5.0s): Timeout for global step increment requests.
- `lock_timeout` (default: 5.0s): Timeout for lock acquire/release requests.

**Use case**: 
- Increase timeouts for slow networks or large model weights.
- Decrease timeouts to fail faster and detect issues sooner.
- `download_weights_timeout` should be large enough for model size (e.g., 7B model ~15GB).

### Recommended Configurations

**Small-scale (1-2 GPUs):**
```yaml
update_steps: 64
trainer:
  params:
    accum_steps: 32
orchestrator:
  queue_size: 800
  batch_timeout: 1800.0
sampler:
  params:
    max_pending_samples: 1600
```

**Medium-scale (4-8 GPUs):**
```yaml
update_steps: 128
trainer:
  params:
    accum_steps: 64
orchestrator:
  queue_size: 1600
  batch_timeout: 3600.0
sampler:
  params:
    max_pending_samples: 12800
```

**Large-scale (16+ GPUs):**
```yaml
update_steps: 256
trainer:
  params:
    accum_steps: 128
orchestrator:
  queue_size: 3200
  batch_timeout: 7200.0
sampler:
  params:
    max_pending_samples: 25600
```

## Scaling & Batch Rules
- Global batch: `global_batch = per_gpu_batch × world_size × accum_steps × (update_steps / num_trainers)`
- Keep global batch constant when moving 1→2 GPUs: halve `accum_steps` or `update_steps`
- Increase global batch with more GPUs: keep or increase `accum_steps` and `update_steps`
- Note: increasing per-GPU batch can cause OOM; distributed does not increase per-GPU memory.

## Key Files
- `ralo/orchestrator.py`: HTTP server wiring the modular components.
- `ralo/orchestrator_components/*`: `ProblemProvider`, `SampleQueueManager`, `GradientAggregator`.
- `ralo/ralo.py`: sampler + trainer algorithms (now using `SamplerClient` / `TrainerClient`).
- `ralo/sampler/client.py`, `ralo/trainer/client.py`: reusable HTTP helpers.
- `ralo_cli.py`: CLI (`orch`, `gen`, default training command) and dataset bootstrapper.

## Environment Variables
- `ORCH_SERVER` (base URL of the orchestrator; shared by trainers/samplers).
- `ORCH_HOST` / `ORCH_PORT` (override bind address when running `ralo_cli.py orch`).
- `GEN_DEVICES` (comma-separated GPU ids for sampler mode).
- `OMP_NUM_THREADS` (default 32).
- `TORCH_NCCL_ASYNC_ERROR_HANDLING=1` (recommended for multi-GPU trainers).
- `WANDB_PROJECT`, `WANDB_RUN_NAME`, `WANDB_DISABLED` (logger overrides).
- `SAMPLER_ALGO`, `TRAINER_ALGO` (switch registered algorithms on the fly).
- `SKIP_TRAINER_REGISTRATION=1` (skip trainer registration; useful for Slurm/closed environments where handshake fails).

## Configuration & Extensibility
- YAML/JSON configs (see `configs/example.yaml`) drive every component: dataset, sampler/trainer params, wandb metadata.
- `sampler` / `trainer` sections choose an `algorithm` (currently `treepo`) and pass arbitrary `params` to that implementation.
- Algorithms register themselves via `ralo.algorithms.register_sampler_algorithm` / `register_trainer_algorithm`.
- Logging goes through `ExperimentLogger`; swap wandb for MLflow/CSV/etc by providing another logger.

### Orchestrator Timeout Configuration
The orchestrator has several timeout parameters to handle trainer failures and prevent resource leaks:

**YAML Configuration:**
```yaml
orchestrator:
  batch_timeout: 3600.0  # Timeout for batch processing in seconds (default: 3600 = 1 hour)
  queue_size: 1600       # Maximum size of training queue (default: 1600)
  timeout_check_interval: 60.0  # Interval to check for timeout batches in seconds (default: 60 = 1 minute)
  keep_last_versions: 2   # Number of weight versions to keep (default: 2)
```

**Timeout Rules:**
1. **`batch_timeout`**: Maximum time a batch can be processed before being considered timed out. If a trainer dies or hangs, batches will be requeued after this timeout. Default: 3600 seconds (1 hour). Increase this if your training batches take longer to process.
2. **`timeout_check_interval`**: How often the orchestrator checks for timed-out batches. Default: 60 seconds. This should be less than `batch_timeout` to ensure timely detection of failures.
3. **`queue_size`**: Maximum number of samples that can be queued for training. When full, new samples from samplers will be rejected until space is available.
4. **`keep_last_versions`**: Number of weight versions to keep on disk. Older versions are automatically pruned to save disk space.

**Example Configuration:**
For long-running training with large batches, you might want to increase timeouts:
```yaml
orchestrator:
  batch_timeout: 7200.0  # 2 hours for very large batches
  timeout_check_interval: 120.0  # Check every 2 minutes
  queue_size: 3200  # Larger queue for more samples
```

**Timeout Behavior:**
- When a batch times out, it is automatically requeued and can be picked up by another trainer.
- The orchestrator logs: `[ORCH] Requeued timeout batch batch_XXX (elapsed: XXXs)`
- If many batches timeout simultaneously, it may indicate trainer failures or network issues.

## Weights & Problem Sync
- Trainers accumulate gradients locally for `accum_steps` micro-batches, then send **1 gradient upload** to orchestrator.
- Orchestrator's GradientAggregator collects `update_steps` **gradient uploads** (not micro-batches), then performs optimizer step, snapshots weights, and prunes history.
- Samplers/trainers poll `/weights/version` → `/weights/download` to stay current.
- Problem data lives solely in the orchestrator; samplers call `/problem/get` instead of reading datasets locally.

## Evaluation Subsystem

RALO can automatically run evaluations on each new weight version and log results to wandb. The orchestrator schedules evaluation jobs per version and samplers execute them in the background without interrupting training.

Flow (per new version):
- Schedule benchmark jobs (AIME 2024/2025, GPQA, HLE/custom, etc.)
- Sampler claims an eval job → ensures latest weights → generates candidates via vLLM
- Metrics are computed via pluggable metric functions
- Results reported to orchestrator and logged to wandb under `eval/<benchmark>/<metric>`

### Enabling Auto-Eval (Config)
Add this to your YAML:

```yaml
evaluation:
  enabled: true
  schedule: on_version_change   # or manual
  max_parallel_jobs: 1
  devices: [0]                  # optional; empty → share sampler GPUs
  wandb_namespace: "eval"
  benchmarks:
    - name: aime_2024
      loader: builtin:aime_2024
      split: test
      max_items: null
      num_candidates: 1
      prompt_template: builtin:aime_cot
      answer_extractor: builtin:boxed
      metrics: [builtin:accuracy@1, builtin:accuracy@5]
    - name: aime_2025
      loader: builtin:aime_2025
      split: test
      metrics: [builtin:accuracy@1]
    - name: gpqa_diamond
      loader: hf:Idavidrein/gpqa
      config: diamond
      split: validation
      metrics: [builtin:mc_accuracy@1]
    - name: hle
      loader: hf:<your_org/your_hle_dataset>
      split: test
      metrics: [your_pkg.metrics:custom_score]
```

### Built-in Benchmarks
- AIME 2024/2025 (`HuggingFaceH4/aime_20xx`) with boxed answer extraction
- GPQA (`Idavidrein/gpqa`, e.g., `config: diamond`), multiple-choice extraction
- HLE: generic HF adapter; configure dataset id and field names as needed

### Metrics & Plugins
Metric functions are simple Python callables and can be loaded via dotted imports. Built-ins include:
- `builtin:accuracy@K` (e.g., `builtin:accuracy@1`, `builtin:accuracy@5`)
- `builtin:mc_accuracy@1` for multiple-choice
- `builtin:exact_match`

Custom metric example:
```python
def custom_score(preds, refs, **kw):
    correct = sum(int(str(p).strip() == str(r).strip()) for p, r in zip(preds, refs))
    return {"my/custom_accuracy": correct / max(1, len(refs))}
```

### Running & Results
- No new commands: when enabled, evaluation runs automatically on each new version.
- Orchestrator REST: `/eval/job/get`, `/eval/job/claim`, `/eval/job/report`, `/eval/results?version=X`, `/eval/stats`, `/compute/stats`.
- Results are logged to wandb as `eval/<benchmark>/<metric>`; the `wandb_namespace` prefixes the path.

### Resource Control
- `max_parallel_jobs` controls how many concurrent eval jobs can run.
- `devices` pins evaluator to specific GPU(s); when empty, evaluators share sampler GPUs.

### Scaling-Law Fits (ScaleRL inspired)
- For each benchmark/metric, RALO records `(compute, reward)` pairs where `compute` is the aggregated GPU-hours (sampler/actor + trainer/learner + evaluator) reported via `/compute/report`, and fits the ScaleRL sigmoid [[arXiv:2510.13786](https://arxiv.org/html/2510.13786v1)]:

  \[
  R(C) = R_0 + \frac{A - R_0}{1 + \left(\frac{C_{\text{mid}}}{C}\right)^B}
  \]

  where \(R_0\) is the baseline reward, \(A\) the asymptotic ceiling, \(C_{\text{mid}}\) the compute required for half of the attainable gain, and \(B\) the slope that captures how sharp the transition is.
- Fitted parameters and loss are logged to wandb under keys like `eval/<benchmark>/<metric>/fit_r0`, `fit_a`, `fit_c_mid`, `fit_b`, `fit_loss`.
- The orchestrator exposes `/eval/fit` (JSON) and writes `eval_fit_summary.json` (now including compute totals and per-version GPU-hour snapshots) so you can monitor convergence and extrapolate future returns from current compute.
- Use `evaluation.shutdown_timeout_sec` to ensure the orchestrator waits for final evaluations (and curve fitting) before shutting down at the end of a run.

### GPU-hour Telemetry
- Sampler, trainer, and evaluator workers periodically call `/compute/report` with their GPU-seconds + token counts; adjust `sampler.params.compute_report_interval` / `compute_report_token_threshold` and the trainer equivalents to control flush frequency.
- Inspect live aggregates via `/compute/stats`, `eval_fit_summary.json`, or wandb keys such as `eval/<benchmark>/compute_gpu_hours` and per-role `compute_{actor|learner|evaluator}_hours`.

## Logging & Metrics

### Trainer Output
The Trainer produces the following output:

**Initialization:**
- `[TRAINER] Registered with orchestrator at http://...` - Successfully registered with orchestrator
  - **Why registration is needed:**
    1. **Initial weight synchronization**: Trainer receives the latest weight version from the orchestrator and synchronizes the local model at startup.
    2. **Orchestrator connection verification**: Verifies that the orchestrator is running and the communication path is healthy.
    3. **Configuration exchange**: Receives orchestrator settings like `update_steps` for use in the training loop.
    4. **Worker identification**: Registers `worker_id` to distinguish multiple trainer nodes.
- `[TRAINER] Updated local weights to version X` - Initial weights downloaded

**During training loop (rank 0 only):**
- **tqdm progress bar**: `Gradient Step: X: 100%|████████| 50/50 [01:23<00:00, 1.66s/it]`
  - Updates on each gradient upload
- `[TRAINER] Updated local weights to version X` - Automatically downloads when orchestrator updates to a new version

**Errors/Warnings:**
- `[TRAINER] Failed to reach orchestrator (attempt X/10), retrying in Xs...` - Retries on orchestrator connection failure
- `[TRAINING PROC] failed to fetch global step: ...` - Failed to fetch global step
- `[TRAINER] Failed to download weights version X from orchestrator.` - Weight download failure

**On shutdown:**
- `[TRAINER] Orchestrator signaled stop (global_step: X)` - Received stop signal from orchestrator
- `[TRAINER] Sent stop signal to sampler workers` - Sent stop signal to samplers

**Note:**
- In DDP environments, each rank outputs independently, so only rank 0 shows the progress bar.
- Gradients are sent to the orchestrator; the trainer does not perform optimizer steps (handled centrally by the orchestrator).

### Sampler Output
The Sampler (vLLM generation workers) produces the following output:

**Initialization:**
- `START vLLM generation...` - Generation workers starting
- `[SAMPLER] Starting problem fetcher from orchestrator at http://...` - Problem fetcher thread started
- `[GEN X] Generation worker process uses GPU Y` - Worker process initialized on GPU
- `[GEN X] CUDA_VISIBLE_DEVICES: ...` - GPU visibility configuration
- `[GEN X] PID: ...` - Process ID
- `[VLLM PROC X] Initialized with weights version Y from server` - Initial weight version loaded
- `[VLLM PROC X] Could not fetch initial version from server: ..., starting with version -1` - Failed to fetch initial version (non-fatal)

**During generation:**
- `[SAMPLER] Fetched X problems (queue size: Y)` - Problem fetching progress (every 100th problem)
- `[VLLM PROC X] newer weights available: Y > Z, updating workers directly from server...` - New model version detected, updating
- `[VLLM PROC X] model updated to version Y. Results: ...` - Model weights successfully updated
- `[VLLM PROC X] weight apply failed (attempt X/Y). Results: ...` - Weight update failed, retrying
- `[VLLM PROC X] ERROR applying model (attempt X/Y): ...` - Error during weight update
- `[VLLM PROC X] giving up updating to Y for now; will retry later. Last results: ...` - Weight update failed after retries
- `[VLLM PROC X] server reported 404 for weights vY; retry later` - Weight version not found on server

**On completion:**
- `[GEN X] Generation worker finished, sending end signal to orchestrator ...` - Worker finished, sending end signal
- `[GEN X] Failed to send end signal: ...` - Failed to send end signal (non-fatal)
- `[SAMPLER] Received end signal from orchestrator after X problems` - All problems processed, shutting down

**Errors/Warnings:**
- `[SAMPLER] Error requesting problem from orchestrator: ...` - Problem fetch error, retrying
- `[SAMPLER] Unexpected response: ...` - Unexpected response from orchestrator
- `[GEN X] pending samples too many, wait for training process ...` - Too many pending samples, waiting
- `[GEN X] Error in generation worker: ...` - Error in generation worker
- `[GEN X] Dropping batch after X retries.` - Batch dropped after retries
- `[GEN X] Failed to requeue batch: ...` - Failed to requeue batch
- `[GEN MONITOR] Dropping batch X after Y retries (timeout).` - Batch timeout, dropped by monitor
- `[GEN MONITOR] Requeued timed-out batch X (retry Y).` - Timed-out batch requeued
- `[GEN MONITOR] Failed to requeue batch X: ...` - Failed to requeue timed-out batch

**Note:**
- Multiple generation workers run in parallel (one per GPU specified in `gen_device`).
- Each worker independently fetches problems, generates samples, and uploads them to the orchestrator.
- Workers automatically update model weights when new versions are available from the orchestrator.
- The problem fetcher maintains a small buffer (default: 20 problems) to avoid pre-loading the entire dataset.

### Orchestrator Output
The Orchestrator displays training progress as follows:

**Initialization:**
- `[ORCH] Problem queue initialized with X items` - Problem queue initialized
- `[ORCH] Saved initial weights as version 0` or `[ORCH] Loaded weights version X from disk` - Weight initialization/loading
- `[ORCH] Orchestrator server started on http://...` - Server started
- `[ORCH] Status reports every 30 seconds. Use /stats endpoint for detailed info.` - Periodic status reporting enabled

**During training (periodic reports, every 30 seconds):**
- `[ORCH STATUS] Step: X | Gradients: Y (pending: Z/W, P%) | Queue: Q samples | Processed: A/B batches | Problems: C/D | Version: V`
  - **Step**: Current global step
  - **Gradients**: Total gradients collected, pending batches, progress percentage
  - **Queue**: Sample queue size
  - **Processed**: Processed/total batches
  - **Problems**: Distributed/total problems
  - **Version**: Current model version

**On gradient upload:**
- `[ORCH] Gradient received from worker_id (step X) | Pending: Y/Z (P%) | Total: N gradients | Queue: Q samples` - Gradient received
- `[ORCH] ✓ Optimizer step completed -> version X | Total gradients: Y | Global step: Z` - Optimizer step completed

**Sample/batch processing:**
- `[ORCH] Sample uploaded | Queue: X samples | Total enqueued: Y` - Sample uploaded (every 10th)
  - **Queue**: Current number of samples in the training queue (controlled by `orchestrator.queue_size`)
  - **Total enqueued**: Total number of samples uploaded since start (cumulative)
  - **Relationship**: `Queue` ≤ `queue_size` (when queue is full, samplers pause via `max_pending_samples`)
- `[ORCH] Batch dispatched to trainer | Queue: X | Processing: Y | Total dequeued: Z` - Batch dispatched (every 5th)
  - **Queue**: Remaining samples in queue after dispatch
  - **Processing**: Number of batches currently being processed by trainers
  - **Total dequeued**: Total batches dispatched to trainers (cumulative)

**Gradient upload (chunked):**
- `[ORCH] Gradient chunks: X/Y (Z%)` - Progress of chunked gradient upload
  - Shows upload progress when gradients are split into chunks (large models)
  - Controlled by `chunk_size_mb` parameter (default: 50MB per chunk)
- `[ORCH] /gradient/upload_finalize called!` - Final chunk received, starting reassembly
- `[ORCH] Reassembled gradient: X chunks → YMB` - Gradient reassembled from chunks
- `[ORCH] Processing reassembled gradient from worker_id (step X)` - Processing gradient from specific trainer
- `[ORCH] Gradient received from worker_id (step X) | Pending: Y/Z (P%) | Total: N gradients | Queue: Q samples`
  - **Pending**: Current pending gradient uploads / `update_steps` required for optimizer step
  - **Total**: Total gradient uploads received since start
  - **Queue**: Current sample queue size

**On shutdown:**
- `[ORCH] Received end signal from sampler` - End signal received from sampler
- `[ORCH] Queue empty with X pending batches; finalizing optimizer step` - Final optimizer step

**Detailed information:**
- Use `/stats` endpoint for real-time status:
  - `global_step`, `pending_batches`, `total_gradients`, `queue_size`, `current_version`, etc.

## HTTP Request Timeout Configuration

All HTTP request timeouts are configurable via YAML under `orchestrator` section. These timeouts control how long clients wait for orchestrator responses before raising errors.

### Timeout Parameters

| Parameter | Default | Description | When to Increase |
|-----------|---------|-------------|------------------|
| `get_batch_timeout` | 60.0s | Trainer batch fetch timeout | If orchestrator is busy with optimizer steps |
| `send_gradients_timeout` | 300.0s (5min) | Gradient upload timeout | For very large models or slow networks |
| `download_weights_timeout` | 600.0s (10min) | Weight download timeout | For large models or slow networks |
| `upload_samples_timeout` | 300.0s (5min) | Sample upload timeout | For large batches or slow networks |
| `fetch_problem_timeout` | 10.0s | Problem fetch timeout | If orchestrator is heavily loaded |
| `register_timeout` | 10.0s | Trainer registration timeout | If orchestrator startup is slow |
| `stats_timeout` | 5.0s | Stats endpoint timeout | Usually sufficient |
| `heartbeat_timeout` | 2.0s | Heartbeat timeout | Usually sufficient |
| `version_check_timeout` | 5.0s | Version check timeout | Usually sufficient |
| `next_step_timeout` | 5.0s | Next step request timeout | Usually sufficient |
| `lock_timeout` | 5.0s | Lock acquire/release timeout | Usually sufficient |

### Example Configuration

```yaml
orchestrator:
  # ... other settings ...
  get_batch_timeout: 120.0  # Increase if seeing timeout errors during optimizer steps
  send_gradients_timeout: 600.0  # Increase for very large models
  download_weights_timeout: 1200.0  # Increase for large models on slow networks
```

### Timeout Error Handling

- **Trainer `get_batch()` timeout**: Trainer will retry with exponential backoff. Increase `get_batch_timeout` if orchestrator is busy with CPU optimizer steps.
- **Gradient upload timeout**: Trainer will retry. Increase `send_gradients_timeout` for large models or slow networks.
- **Weight download timeout**: Trainer will retry. Increase `download_weights_timeout` for large models.

## Troubleshooting
- OOM: lower per-GPU batch, increase `accum_steps`, shorten sequences, use bf16/FP16, set `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True`
- Communication: open ports/firewall, set `TORCH_NCCL_ASYNC_ERROR_HANDLING=1`
- Global loss: add all-reduce of detached loss and print on rank-0
- **Slurm/Closed environments**: If trainer registration fails due to network restrictions:
  - Registration failures now only print warnings and training continues (default behavior)
  - To skip registration entirely, set `SKIP_TRAINER_REGISTRATION=1`
  - Gradient uploads and weight downloads work normally even if registration fails
- **Timeout errors**: If seeing `ReadTimeoutError` or `HTTPConnectionPool timeout`:
  - Increase relevant timeout values in `orchestrator` section of YAML config
  - Check orchestrator logs for CPU optimizer step duration (may block HTTP requests)
  - Consider increasing `get_batch_timeout` if orchestrator is frequently busy

## Multilingual Support (Korean/English)

RALO supports multilingual training with Korean and English text. The tokenizer automatically handles both languages based on the model's training data.

### Using Korean/English Models

1. **Model Selection**: Use a multilingual model (e.g., models trained on Korean and English data) or a Korean-specific model:
   ```python
   model_path = "your-korean-english-model-path"
   ```

2. **Prompt Template**: The default prompt template in `ralo_cli.py` uses `apply_chat_template()` which automatically handles multilingual input:
   ```python
   def make_prompt_fn(self, item):
       return self.tokenizer.apply_chat_template(
           [
               {"role": "system", "content": system_prompt},
               {"role": "user", "content": item["Q"]},  # Can be Korean or English
           ],
           tokenize=False,
           add_generation_prompt=True,
       )
   ```

3. **Custom Prompt Function**: To customize prompt formatting for Korean/English:
   ```python
   ralo = RALO(model_path="...", ...)
   
   def custom_prompt_fn(self, item):
       # Handle Korean/English prompts
       question = item.get("Q", "")
       # Your custom formatting logic here
       return formatted_prompt
   
   ralo.set_rollout_prompt_fn(custom_prompt_fn)
   ```

4. **Tokenizer Configuration**: The tokenizer is automatically loaded from the model path. For Korean-specific tokenization:
   - Ensure your model was trained with Korean tokenizers (e.g., SentencePiece, BPE with Korean vocabulary)
   - The tokenizer will automatically handle Korean characters if the model supports it

5. **Dataset Format**: Your dataset should contain Korean or English text in the `Q` (question) and `A` (answer) fields:
   ```python
   {
       "Q": "한국어 질문 또는 English question",
       "A": "한국어 답변 또는 English answer"
   }
   ```

### Notes
- The model's tokenizer determines language support. Check your model's documentation for supported languages.
- vLLM automatically uses the model's tokenizer, so no additional configuration is needed for generation.
- For best results, use models specifically trained on Korean-English data or fine-tuned for your target languages.

## License
Follows the project root's license.
